%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Development Goals and Design Decisions}\label{chp:designgoals}

\GrG~is the successor of the \textsc{GrGen} tool presented at ICGT 2006~\cite{GBGHS:06}.
The ``.NET'' postfix of the new name indicates that \textsc{GrGen} has been reimplemented in C\# for the Microsoft .NET or Mono environment~\cite{NET,MONO};
it is open source licensed under LGPL3(\url{www.gnu.org/licenses/lgpl.html}) and available for download at \url{www.grgen.net}.

Over the course of time, it moved from high-performance fixed-shaped patterns and limited sequences for control to flexibly-shaped patterns (with nested and subpatterns) and very high programmability (for one with the sequences and for the other with an integrated imperative programming language), while retaining its performance.

\GrG\ offers development at the abstraction level of graph representations.
It offers the convenience of linguistic abstraction, with dedicated syntax for graph processing, and static type checking.
In contrast to limited libraries or the very leaky abstractions often offered by common programming frameworks.
The abstraction is not halting at the languages, but carried through to development support.
With a debugger showing the data and the processing effects on that very level,
removing a major shortcoming of many domain-specific languages that are linguistically better but are undebuggable at their level.
And with performance optimization helpers operating at this level, in the form of search plan explanation of the pattern matchers, as well as profiling instrumentation and statistics printing for the search steps.

The main characteristics and development goals of \GrG\ have been:
\begin{itemize}
	\item productivity through declarative pattern-based rules of very high expressiveness
	\item high-performance pattern matching and graph rewriting, based on a model implementation tailored for it, and an optimizing compiler
	\item general-purpose graph-representation processing enabled by its very high programmability, and customizability
	\item debugability: the processing state and esp. the graph can be inspected, esp. visually
\end{itemize}

%The main guidline during development was:
Whenever a new feature was found to be needed, we asked ourselves: can this be made more \emph{general}, still keeping it \emph{understandable} and \emph{efficiently} implementable.
 
In the following, we'll take a deeper look at the development goals of \GrG\ (comparing it to competitor tools on the way).

\section{Expressiveness}
is achieved by \emph{declarative} specification languages for \emph{pattern matching} and \emph{rewriting} that are bursting with features, building upon a rich graph model language.

In addition to the unmatched expressiveness of the single-element operations offered by the pattern and the rewrite parts,
are \emph{nested} and \emph{subpatterns} available for combining them flexibly.
They allow to process substructures of arbitrary depth and breadth with a single rule application.
This surpasses the capabilities of the VIATRA2\cite{viatra2,recursiveviatra} and GROOVE \cite{Groove} tools, our strongest competitors regarding rule expressiveness.
You may have a look at the GrGen.NET solution of the program understanding case \cite{ProgramUnderstanding} of the TTC 2011 highlighting how \emph{concise} and \emph{elegant} solutions become due to the expressiveness of the language constructs.

While the patterns can be combined in a functional way with nested and subpatterns to build complex rules, can the rules be combined in an imperative way with graph rewrite sequences, executing a graph state change after the other.

The sequences used for orchestrating the rules are at the time of writing the most advanced \emph{strategy language} offered by any graph rewriting tool, featuring variables of elementary as well as container types (called storages when containing nodes, as pioneered by the VMTS\cite{vmts} tool), and sequential, logical, conditional, and iterative control flow as the base operations, plus sequence definitions and calls.
Small graph-changes and computations can be executed with sequence computations without the need to enter and exit again the rule layer.
The most notable feature of the sequences are their \emph{transactional} and \emph{backtracking constructs}, which support search space crawling and state space enumeration with their ability to roll changes to the graph back to an initial state, saving you a considerable amount of change reversal and bookkeeping code you'd need for implementing this on your own.

Expressiveness is achieved by language elements that were designed to be \emph{general} and \emph{orthogonal}.
\GrG\ offers the more general iterated patterns subsuming the multinodes as they are common for graph rewriting tools.
\GrG\ offers the more general recursive patterns subsuming the regular path expressions which are the more common choice.
You get means to \emph{combine patterns}, instead of dedicated solutions for single nodes or edges.
\GrG\ offers generic container types, and not only storages, as is more common (non-container-variables are even more common), and it offers them as graph element attributes, too and not only for processing variables.
\GrG\ offers not only node and edge types, but also a graph type allowing to store subgraphs; and especially does it allow that graph element attributes are typed with it.
You can work with a type system as you know it from object-oriented-languages (where types can be used freely in defining other types).

Expressiveness is not for free, its price is \emph{increased learning effort}.
\GrG\ offers the biggest toolbox for graph rewriting that you can find, with many tools and high-powered tools.
For nearly any graph-representation-based task do you find a collection of tools that allow you to handle it in an adequate way, leading to a \emph{concise solution} and \emph{short development} and \emph{maintenance durations} and thus costs.
But you have to learn these tools and how to use them beforehand.
For small tasks that effort is likely higher than the net gains in the end.

For subtasks for which the declarative pattern-based rules do not work well (this happens occasionally), are you free to revert to \emph{imperative} and \emph{object-oriented programming} as known from the C++, Java, C\# language family, utilizing the imperative parts of the \GrG-languages.
You loose a good amount of the benefits of employing the tool then (we consider model transformation tools offering own -- but only imperative -- languages pointless (and OCL-expressions are only a small improvement in this regard)), but neither are you stuck, nor are you forced to convoluted workarounds, as it may happen with tools that offer only pattern-based rules, e.g. Henshin\cite{Henshin} or AGG\cite{agg}.
The \emph{multi-paradigm languages} of \GrG\ offer you all the constructs you need for transforming graph-representations.

\section{General-Purpose Graph Rewriting}
in contrast to \emph{special-purpose} graph rewriting requires high \emph{programmability}, \emph{genericity} and \emph{customizability}.

Some graph based tools are geared towards special application domains, e.g. biology (XL \cite{xl} or verification (GROOVE \cite{Groove}).
This means that design decisions were taken that ease uses in these application areas, at the cost of rendering uses in other domains more difficult.
And that features were added in a way that just satisfies the needs of the domain at hand and its tasks, instead of striving for a more general solution (which would have caused higher costs at designing and implementing this features).

This can be seen well in the approach towards state space enumeration. 
Instead of offering a built-in fixed-function state space enumerator like GROOVE and Henshin do, are you given the language devices needed to program one with a few lines of code, with the backtracking abilities of the sequences, and subgraph extraction, comparison, and storing in variables/graph element attributes.
This highlights the costs of being general-purpose: you \emph{must program} what you need.
But it also highlights the benefits: you \emph{can program exactly} what is needed, being very \emph{flexible} in how state-space enumeration is carried out.
The former tools expect a rule set which they apply exhaustively to create a state space, one state per rule application. 
But state space enumeration requires a lot of memory due to the state explosion phenomenon -- for practical applications you need control over which changes are to be seen as states and which not, and single rules often are too low-level regarding this classification, materializing many superfluous states.
\GrG\ gives you that kind of control (backtracking rule application and state materialization are separate things), allowing you to unfold deeper given the same amount of memory. 
When it comes to non-toy-examples, \emph{programmability wins} over coarse-grain \emph{fixed-function} units.
But of course only as long as you don't need to reinvent the wheel, when you can program based on a \emph{suitable level of abstraction}.

The backtracking constructs are useful in crawling a search space, too, where the reached states don't need to be materialized into the graph, and doing so would be inadequate.
When it comes to usability in many contexts, for many tasks, again programmability wins over coarse-grain fixed-function units.
This should not hinder you to choose the latter when your task does not require more, they are easier to use, but beware of the day when the requirements change and you hit a wall.
The approach of \GrG\ is to offer you \emph{languages} to achieve your goals, not \emph{pre-coded solutions} that may or may not fit to your task at hand.
This holds not only for the languages used to describe the data or to specify the computations, but also for the visualization and the debugger. 
They can be customized easily to the characteristics of your graph representation. 

Programmability always must come with means to abstract and encapsulate the programs into own units and parametrize them, so you are not exposed to details where you don't need them (but can access them when you need to do so, in contrast to opaque fixed-function units).
They are available with the rules and tests, the subpatterns, the procedures and functions including method procedures and method functions, the filter functions, and sequence definitions; and the input and output parameters available for them.

While the old \textsc{GrGen} started as a special-purpose compiler construction tool for internal use with fixed-shape patterns 
(optimizations on the graph based compiler intermediate representation FIRM -- see \url{www.libfirm.org}),
was the new \GrG\ built from the beginning as a general-purpose graph transformation tool for external use
-- to be employed in areas as diverse as computer linguistics, engineering, computational biology or software engineering --
for reasoning with semantic nets, transformation of natural language to UML models,
model transformation, processing of program graphs, genome simulation, or pattern matching in social nets or RDF graphs.
Have a look at \cite{usecomputerlinguistics} or \cite{usemodeltransformation} or \cite{usegeneexpression} for some of the results.
Or at the results of \GrG\ for the diverse tasks posed in the transformation tool contests. 
It was always amongst the best rated tools, a record that is only achievable for a general-purpose tool with expressive and universally usable languages.
%which offers the highest combined speed of development and execution for graph based algorithms through its declarative languages with automatic optimization.

\section{Performance}
i.e. high speed at modest memory consumption, is needed to tackle real world problems.
It is achieved by an \emph{optimized model implementation} that is tailored for efficient and scalable pattern matching and rewriting,
while not growing memory needs out of proportion -- as it happens for incremental match engines storing and maintaining the matches of all rules at any time (as offered by VIATRA2\cite{viatra2}).
The nodes and edges are organized in a system of ringlists, giving immediate access to the elements of a type, and to the incident elements of an element.
When needed, attribute indices or incidence count indices can be applied, giving fast access to graph elements based on attribute values, or incidence counts.

The generative approach and its \emph{compilation} of the rules into executable code is helping tremendously regarding performance, employing nested loops for matching the pattern elements (and a recursive descent based pushdown machine for combining the patterns).
But it comes at a cost: the test-debug-cycle is slowed down compared to an interpreter, and you are less flexible regarding runtime changes.

A further help regarding performance are the \emph{types}, which are speeding up the pattern matcher
-- besides being a help in modeling the domain, and besides easening your life by eliminating large classes of errors at compile time.

Performance is further fostered by \emph{explicit control} with sequences, and rule applications from \emph{preset parameters} on, defining where to match the rules (giving rooted pattern matching), and which rule to match when (getting faster to a rule that matches, based on domain knowledge), in contrast to approaches based on implicit control, most notably the graph-grammar approach with its parameterless rules (only controlled by an optional layering). 

Performance is gained by the \emph{host graph sensitive search plans}.
In order to accelerate the matching step, we internally introduce \newterm{search plan}s
to represent different \newterm{matching strategies} and equip these search plans with a cost model, taking the present host graph into account.
The task of selecting a good search plan is then considered an optimization problem~\cite{BKG:07,Bat:06}.
In contrast to systems like Fujaba\cite{fujaba,fuj}, our strongest competitor regarding performance,
is our pattern matching algorithm fully automatic and neither needs to be tuned nor partly be implemented by hand.

Furthermore, several strength reduction and inlining \emph{optimizations} are employed by the compiler to eliminate the overhead of the higher level of abstraction where it is not needed.
In addition, there is the \emph{search state space stepping} optimization available, 
which boosts rule applications executed from inside a loop (under normal circumstances), by starting matching of the next iteration where the previous iteration stopped.
It plays an important role in being at least one order of magnitude faster than any other tool known to us according to \indexed{Varr\'o's benchmark}\cite{varro_bench}.
Finally, search intensive tasks can be \emph{parallelized}, reaping benefits from our-days multicore machines; have a look at our solution for the Movie Database case of the TTC14\cite{MovieDatabase} to see what can be achieved this way.

\section{Understandability and Learnability}
was taken care by evaluating for each language construct several options,
preferring constructs already known from imperative and object-oriented programming languages as well as parser generators ---
the ones which seemed most clean and intuitive while satisfying the other constraints were chosen.
This can be noted in comparison with the languages of the GReTL \cite{GReTL} tool, esp. with its powerful graph query language GReQL,
which may be pleasing to someone from the realm of formal specification, but which are not to the \emph{mind of a programmer}.
%A programmer who just wants to process his graphs on a higher level of abstraction, with declarative pattern matching and rewriting on a visualization of the network of objects -- instead of low level pointer structure fiddling, chasing objects by following references in the debugger.
This can be noted in comparison with functional programming languages, which throw out the baby with the bath in their attempt to control modifyability (the concept of state is more adequate to describe the world we live in, makes things better understandable for our brains, and performs better on our machines), and hamper learnability with their (over-)use of higher order programming.
You may have a look at the GrGen.NET solution of the Hello World! case \cite{HelloWorld} to judge on your own.
As we know that even the best designed language is not self explaining we put an emphasize on the \emph{user manual} currently read by you.
Especially since the consequences of the development goals expressiveness and programmability are inevitably increasing the learning effort.

\GrG\ is a pretty heavyweight beast --- you need to learn some languages in order to use and control it (but easily learnable ones, as explained above).
But thereafter, you are able to develop at the abstraction level of graph representations, and at a speed outpacing by far any developer working with a traditional programming language (for a graph-representation processing task).
A specification change carried out in a day in \GrG\ easily amounts to a man-week or even man-month(!) change in a traditional programming language.

\section{Development Convenience}
is gained especially by the offered \emph{interactive} and \emph{graphical debugging} of rule applications.
The debugger visualizes the matched pattern and the changes carried out by rewriting in the graph where they apply,
for the currently active rule in the sequence (which is highlighted).

While rewrite rule application is indeterministic per se, \GrG\ is implemented so that its behaviour is as deterministic as possible, so that you can attack issues multiple times until they can be solved.

A further point easening development is the \emph{application programming interface} of the generated code,
which offers access to named, statically typed entities, catching most errors before runtime and allowing the code completion mechanisms of modern IDEs to excel.
In addition, a generic interface operating on name strings and .NET objects is available for applications where the rules may change at runtime (as e.g. the \GrShell).

The API allows you to lookup graph elements by type, and to fetch all incident elements of an element, even in reverse direction, laying the foundation for graph-oriented programming.
In contrast to traditional programming that operates in passes over graph-representations from some root objects on, without direct access to elements inside the data structure. 
And with only the ability to follow an outgoing edge from its source node on, which means to dereference a pointer that points to the target.
Every node supports an unbounded number of incident edges, increasing flexibility and regularity compared to some statically fixed pointer fields as utilized normally.

There's one convenience not offered you might expect: a visual rule language and an editor.
We're following a first things first policy here, and graphical debugging is a must-have, while a graphical rule editor is only nice-to-have 
(we consider tools offering the other way round mere toys, created by people without own experience in graph-representation processing).% just praying at the altar of some visual programming theory idea.
Besides, textual languages bring some benefits on their own: graph transformation specifications to be processed by \GrG\ can be easily \emph{generated}, stored in a source code management system, and \emph{textually diff'ed}.
A graphical editor can be implemented on top of the \GrG\ languages, reading and writing their textual serialization format --- volunteers are welcome.
Textual languages are also a good deal cheaper to implement.
Given the limited resources of an university or open-source project this is an important point,
as can be seen with the AGG\cite{agg} tool, offering a graphical editor but delivering performance only up to simple toy examples
--- unfortunately causing the wrong impression that graph rewriting is notoriously inefficient.

\section{Well Founded Semantics}
to ease formal, but especially human reasoning.
The semantics of \GrG, or better a subset of the current \GrG\ are specified formally in \cite{DissRuby}, based upon graph homomorphisms, denotational evaluation functions and category theory.
The \GrG-rewrite step is based on the \newterm{single-pushout approach} (SPO, for explanation see~\cite{spoapproach}).
The semantics of the recursive rules introduced in version 2.0 are sketched in \cite{Jak:08},
utilizing pair star graph grammars on the meta level to assemble the rules of the object level.
The formal semantics were outpaced by development, though, if you want to reason formally you have to restrain yourself to a subset of the current \GrG.

But you may just prefer the simple graph programming language GP\cite{gp} in this case, which was developed with formal reasoning in mind.
For us, the convenience at using the language had priority over the convenience at reasoning formally about the language.
In order to achieve simple reasoning, you need to (over-)simplify the languages to a level that makes using them much harder, as solutions need to be expressed with more, and more convoluted code.
You see this struggle in the programming languages community in theory people asking for the ban of the break and continue control flow constructs, although code using them is clearer and better readable.
They do so because the constructs make proving propositions about the code much harder.
You see this struggle in the graph rewriting community in the preference of the \newterm{double-pushout} approach (DPO, for explanation see~\cite{dpoapproach}) by the theory people, although rules using them require convoluted coding for a simple node deletion.
We optimized our languages for people that need to get real work done in them, not for people who want to prove that the program is up to a specification coded in predicate or some other kind of logic. 

\section{Platform Independence}
is achieved by using languages compiled to byte code for virtual machines backed by large, standardized libraries, specifically: Java and C\#.
This should prevent the fate of the grandfather of all graph rewrite systems, PROGRES\cite{schuerr99progres},
which achieved a high level of sophistication, but is difficult to run by now, or simply: outdated.

Java is only needed for development, the generated code and its supporting libraries are entirely .NET based.
The .NET framework allows to use native code, so if you really need to, you can integrate via .NET - native interop a graph rewrite system into some native application.

There's another trait to platform independence: \GrG\ is built on its own model layer, in contrast to the Java model transformation world, where EMF has emerged as a quasi-standard.
Don't expect this to change, the performance of declarative pattern matching and rewriting is our top concern, and that can be best achieved with a model optimized for this task (the current type and neighbourhood ringlist implementation is efficient and scalable, and supports the search space stepping optimization; directed edges are always navigable in both directions, giving the planner more room for choosing a good search plan; and some fields are included that render isomorphy checking cheap and supply some cost-free visited flags).

EMF is not that important for us for another reason: we consider the MDA vision an illusion.
The costs of developing a domain-specific language, a model as a high-level representation of a domain, and a code generator emitting executable code from it (maybe with an intermediate platform dependent model in the chain) are considerable and can only be amortized over many use cases or for large program families, with a high degree of commonalities (otherwise there's nothing domain-specific to exploit) and a high degree of variation (otherwise there would be no real benefit compared to direct coding).
Model-based development of single applications is economical nonsense, no matter what the claimed benefits are, the costs will never pay off.
So only large program families are left, which are few, or languages for certain infrastructure tasks that are common among multiple applications, which requires a general-purpose language for that specific domain so it can be adapted to the exact context, leading to further increased costs (as can be seen in the feature development of the GrGen language, a domain-specific language for graph-representation processing...).
We assume there are a good deal more use cases existing for graph representation processing than for model-based development.

\section{General-Purpose Graph Transformation}
in contrast to \emph{model} transformation.
Several model transformation tools offer graph pattern matching as a means of transformation and graph-based tools have been successfully used to model many domains; a clear separation is not possible then \cite{Jakumeit2013}.
But pattern matching is typically limited in dedicated model transformation tools, as is their performance regarding it, esp. their ability for achieving automatically high-performance solutions --- the EMF poses an obstacle to efficient automatic-declarative pattern matching.
Often, model transformation tools offer just \emph{mapping single elements} of a source model (inspecting some local context via OCL-expressions) to some target elements, which is only sufficient for simple tasks.
Those tools -- e.g. ATL\cite{atl}, to a lesser degree Epsilon\cite{epsilon} -- typically offer only \emph{batch-wise offline} mapping of one graph-like representation to another one.
\GrG\ works well for model transformation task, esp. due to its compiler construction roots; so feel free to use it if you are one of the few that indeed benefit directly from the model driven architecture approach or are interested in supplying corresponding models and languages to the benefit of others. 

Furthermore, graph \emph{transformation} in contrast to graph \emph{databases}.
The focus is on efficient \emph{pattern-based} matching, and rewritings that are strongly coupled to the query results, capable of executing fine-grain spot-wise changes (with transaction support for crawling search spaces). 
Executed on an \emph{in-memory} graph-structure, by a \emph{single user} at a time.
In contrast to e.g. Neo4J\cite{neo}, that offers non-pattern queries that are feeding loosely coupled simple updates via a query-result container in between, which is accumulating the matches but renders fine-grain changes based on the exact matches/spots difficult.
Executed on a \emph{backing-store}, by \emph{multiple users} that are working concurrently at the same time, isolated by transactions.

So \GrG\ is not suited for scenarios where many users need to access data at the same time. 
It is very fast, so you can handle multiple-user scenarios simply by serializing the accesses --- this could be well sufficient for your tasks, but won't scale towards really large numbers of users.
You may use it as an application-embedded database, as
\GrG\ \emph{does} offer \emph{online} modification of graph-structures,
but persistence of online modifications is only offered via recording to a change log that needs to be replayed at next program start
(besides writing a full dump from memory at session end, as it is common to batch-wise offline processing).

\GrG\ is not suited for scenarios where the limits of main memory are exceeded.
A well equipped single sever with 128GB of main memory is capable of storing somewhat under about 500 Million nodes plus 1 Billion edges (naked, without attributes and without names), see Chapter \ref{cha:performance} for the maths.
This should be sufficient for many tasks, but still there are task that grow beyond it.
Besides, other things come into play at that sizes: importing such a graph will take some time, maybe longer than you're willing to wait -- the GRS importer is munching at some ten thousand elements per second.
Databases that work on a backing store and only shuffle data visited in are at an advantage here (amortizing import cost over the course of processing).
\GrG\ scales very well (much better than the tools of the authors from the model transformation community that write papers about scalability) and can easily handle millions of elements, matching a pattern in such a graph within the blink of an eye, but it does not contain dedicated optimizations for very huge datasets.

\GrG\ is a tool that was built for achieving goals in practice in mind.
Despite a shared pair graph grammar inspiration is it unrelated to triple graph grammars and their implicit but unrealistic assumption that typically one graph representation is to be transformed to another, basically equivalent, just somehow a bit differently formulated graph representation, without information loss or enrichment in either direction (bidirectional transformation tasks are very seldom).
\GrG\ can be used for meta-programming (esp. due to its compiler construction roots), but otherwise do you work with it at the object and class level, not at the meta, meta-meta, or meta-meta-meta tralala-ding-ding-dong level (the abstraction levels of the true model-based software engineers).
%(Besides, \GrG\ is worth hundreds of model driven engineering papers, we just considered it more important to produce something usable and executable, compared to mostly hot air dominating that community.)
When you don't care about expressiveness and performance for real world tasks, but are just interested in pimping your thesis, pick AGG and do some critical pair analysis.
\GrG\ is the most sophisticated implementation of algebraic graph transformation, but it was not created in order to showcase that the theory can be made executable.
Neither was it created to prove a visual programming point.
\GrG\ was created because we know from our own experience that a great graph processing language and a supporting environment with visual debugging are of real help for graph-representation-based tasks.
