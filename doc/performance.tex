\chapter{Performance Optimization}\label{cha:performance} \indexmain{performance}\indexmain{optimization}

The most important point to understand when optimizing for speed is that the expensive task is the search carried out during pattern matching. The effort of rewriting (the dominant theme in graph rewriting literature) is negligible.

Searching is carried out with a backtracking algorithm following a search plan in a fixed order, 
binding one pattern element after another to a graph element, checking if it fits to the already bound parts.
If it does fit search continues trying to bind the next pattern element (or succeeds building the match object from all the elements bound if the last check succeeds), if it does not fit search continues with the next graph element; if all graph element candidates for the currently focused pattern element are exhausted, search backtracks to the previous decision point and continues there with the next element.

Typically, first a graph element is determined with a lookup operation reaching into the graph, binding the element to a graph element of the required type (the less elements of that type exists, the better) -- then neighbouring elements are traversed following the graph structure (the less neighbouring elements exists, the better), until a match of the entire pattern is found.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Search Plans}

A search plan for the pattern in figure \ref{perf:figpatterntosearch} is:\\
\texttt{lkp(v1:A); out(v1,e1:a); tgt(e1,v2:B); out(v2,e3:b); tgt(e3,v3:C); out(v3,e2:a)}\\
The search operation \texttt{lkp} denotes a node (or edge) lookup in the graph, \texttt{out} follows the outgoing edges of the given source node and \texttt{in} follows the incoming edges of the given target node, while \texttt{src} fetches the source from the given edge and \texttt{tgt} fetches the target from the given edge.

For some graphs the search plan might work well, but for the graph given in figure \ref{perf:figgraphtosearchin} it is a bad search plan.
Why so can be seen in the search order sketched in figure \ref{perf:figbadsearch}.
Due to the multiple outgoing edges of \texttt{v1} of which only one leads to a match it has to backtrack several times.

This schedule in contrast is a a good one:\\
\texttt{lkp(v3:C); out(v3,e2:a); tgt(e2,v2:B); out(v2,e3:b); in(v2,e1:a); src(e1,v1:A)}\\
corresponding to the search order depicted in figure \ref{perf:figgoodsearch}.

It is crucial for achieving high performance to prevent following graph structures splitting into breadth as given in this example, and especially to avoid lookups on elements which are available in high quantities.

\begin{figure}[p]
  \centering
  \includegraphics[width=0.7\textwidth]{fig/Pattern}
  \caption{Pattern to search}
  \label{perf:figpatterntosearch}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=0.7\textwidth]{fig/Graph}
  \caption{Host graph to search in}
  \label{perf:figgraphtosearchin}
\end{figure}

\pagebreak

\begin{figure}[p]
  \centering
  \includegraphics[width=0.7\textwidth]{fig/GraphBad}
  \caption{Bad search order}
  \label{perf:figbadsearch}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=0.7\textwidth]{fig/GraphGood}
  \caption{Good search order}
  \label{perf:figgoodsearch}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Find, Don't Search}
It is better to find elements with certain characteristics straight ahead without search,
utilizing a data structure that allows to tell the elements that follow the characteristic apart from the ones that do not,
than to search for them, traversing each and every node or edge in the graph, subjecting it to a test for the characteristic.

Welcome to indices as known from database parlance.
In \GrG{} the following types of indices are supported:
\begin{enumerate}
	\item Type indices
	\item Neighbourhood indices
	\item Attribute indices
	\item Incidence count indices
	\item Name index
	\item Uniqueness index
\end{enumerate}

You find more on them in Chapter~\ref{cha:indices}.

%-----------------------------------------------------------------------------
\subsection{Consequences For Optimization}

The consequences of the type indices~\ref{sec:typeindices} and the neighbourhood indices~\ref{sec:neighbourhoodindices} for optimization are:

\subsubsection*{Use types!}
Fine grain types cause a fast lookup of the first pattern element(s), or more exactly: they cause a small list of candidate elements for the first pattern element to be tried.
And they cause quicker pruning of search branches because of early failing type checks.
Using fine-grain types is easy in GrGen, as multiple inheritance on node and edge types (cf. \ref{nodeandedgetypes}) is supported.
Besides, the more fine grain the graph is typed, the better are the statistics, allowing \GrG{ } to find better search plans (see more for this below).

\subsubsection*{Prune early}
Attribute conditions are evaluated as soon as all needed graph elements are matched (saving us from enumerating futile match extensions).
But the unit of scheduling are the full attribute condition expressions. 
If an expression can be separated into different parts that depend on less pattern elements than the entire expression (such a separation is trivial for boolean expressions joined by logical conjunctions), split it into those parts.
The parts are then checked earlier in the search process.
You could even introduce checks that are logically not needed because a later check removes all incorrect matches anyway, just to achieve better performance, which is the case if the checks allow to prune some branches from the search space earlier.
This holds as long as the checks are cheap, if they are expensive you could do the opposite and introduce artificial dependencies to all pattern elements instead, to ensure the attribute condition is evaluated as late as possible.

%\pagebreak

\subsubsection*{Prefer directed edges}
Non-directed edges in the pattern are searched in both directions, considerably increasing the search space.
Use non-directed edges only if they you really needed them.
For some problems this is the case, you then simply have to pay the price of the increased effort for the symmetry.
But some problems where undirected edges are more natural can be easily encoded with directed ones -- in case of performance problems, refactor and optimize them to employ only directed edges, imposing an arbitrary but deterministic direction on the edges.
Besides, the vstructure statistics are more discriminating in case of directed edges, leading to better search planning results; in case of undirected ones the information from both directions is coalesced.

%\pagebreak

\subsubsection*{Beware of Disconnected Patterns}
Disconnected patterns cause a combinatorial explosion of the matches, because the overall number of matches equals the cartesian product of the partial matches of the disconnected parts. 

This is inevitable and a price that must be simply paid if this is really needed.
But it is normally not needed, and, most importantly, is not specified accidentally, as a disconnected pattern in a single flat pattern is typically immediately visible.
But take care of nested patterns or subpatterns.
You might overlook that nested patterns and subpatterns are matched strictly one after the other and especially after their containing pattern.
You cannot ignore block borders and subpattern calls, they disconnect otherwise connected components.
Esp. take care of not disconnecting patterns when factoring out a common part into a subpattern to improve the code.
But due to inlining, things look better than what was said until now.

\begin{example}
Take a look at pattern nesting, when the pattern is disconnected, you run into issues due to combinatorial explosion.
This holds for sure for alternative and iterated patterns.

So you must take care of pattern cardinality ...
\begin{grgen}
test bad {
  n1:Node; n2:Node; // builds cartesian product of all nodes in the graph (O(n*n))
  multiple {
    n1 --> n2; // then filters it down to the connected nodes
  }
}
\end{grgen}
... and alternatives ...
\begin{grgen}
test bad {
  n1:Node; n2:Node; // builds cartesian product of all nodes in the graph (O(n*n))
  alternative {
    single {
      n1 --> n2; // then filters it down to the connected nodes
    }
  }
}
\end{grgen}
\end{example}

\begin{example}
Take a look at pattern nesting, when the pattern is disconnected, you run into issues due to combinatorial explosion.
This may hold for independent and subpatterns -- but inlining is of help here.

The edge from the independent in the example is typically inlined into the pattern removing the issue...
\begin{grgen}
test bad {
  n1:Node; n2:Node; // builds cartesian product of all nodes in the graph (O(n*n))
  independent {
    n1 --> n2; // then filters it down to the connected nodes
  }
}
\end{grgen}
... as is the subpattern body inlined into the pattern, removing the performance issue.
\begin{grgen}
test bad {
  n1:Node; n2:Node; // builds cartesian product of all nodes in the graph (O(n*n))
  :P(n1,n2);
}
pattern P(n1:Node, n2:Node) {
  n1 --> n2; // then filters it down to the connected nodes
}
\end{grgen}
Take a look at the output of the \texttt{explain} command to check whether inlining occurred.
\end{example}

The need to take nested pattern borders and subpattern calls into account is due to the recursive descent matching with a multi-pushdown machine as described in Section \ref{matchingflow} and Section \ref{pushdownmachine}.

Subpatterns are matched top-down, from the input parameters on.
If the input arguments are disconnected in the pattern containing the subpattern, the containing pattern enumerates the cross product of the matches of the disconnected parts, which is only later on filtered for the ones which are connected.
This will likely wreak havoc on search performance.
Even if you don't search for all matches, if you only compute a single overall match --- the calling pattern must enumerate a lot of combinations of its parts (worsened by the fact that those are typically found often because of their simplicity), until the nested pattern finally is able to connect one of the disconnected pairs fed into it.
It might be more efficient to just search from a start parameter towards a connected end location, and yield the found one out (cf. \ref{sec:localvarorderedevalyield}); or to search from a start parameter on all connected end locations, collecting the found ones in a result set -- and then to check the ones found alongside connectedness in a second step.

Nested patterns are also matched top-down, from the input parameters on. 
But parameter passing is implicit here, the elements from the containing pattern that are referenced in the nested pattern are passed in automatically as arguments.
This holds especially for the \texttt{alternative} and \texttt{iterated} constructs, which are matched with the pushdown machine (cf. \ref{pushdownmachine}), too, but also for the \texttt{negative} and \texttt{independent} constructs, which are matched with nested local code embedded into the matcher code of their containing pattern.

But don't shy away from using subpatterns or independents too early, \indexed{inlining} is of help here!

The elements from an independent that are linking a disconnected pattern are typically inlined into their using pattern.
The elements from a subpattern are often inlined into their using pattern, causing the pattern to get connected (again), 
but also removing the pushdown machine overhead.
But you must take into account that the \emph{inlining} implemented in GrGen is limited to depth one.
If a pattern is disconnected over two or more levels of subpattern usage (which might happen statically with one subpattern using another subpattern, and will for sure dynamically on a subpattern recursion path), it will hit performance.
You may have a look at the output of the \texttt{explain} command (cf. \ref{custom}) to see if the subpatterns are disconnected.
This is typically indicated by multiple lookups in the containing pattern, 
for fetching the disconnected starting points,
which are then handed down with preset parameters to the nested or subpatterns,
and only get connected there,
with search commands following their outgoing or incoming edges.

%-----------------------------------------------------------------------------
\subsection{Search Planning On Request}
Search planning at runtime is only carried out on request!
You must analyze the graph and then re-generate the matchers manually,
with \texttt{custom graph analyze} and \texttt{custom actions gen\_searchplans} issued on the command line, 
or with calls of the \texttt{Custom} methods of the \texttt{IGraph} and the \texttt{IActions} objects.
Unless you do so, the static search plan is kept in place.

You can display the search plan currently employed with the \texttt{custom actions explain <actionname>} command,
in order to inspect how the pattern elements are matched.
Issue it before search planning to show the statically generated search plan, issue it afterwards to show the dynamically re-generated search plan.
See subsection \ref{custom} for more on this.

The initial static search plan typically starts with a lookup of an arbitrary edge, and then arbitrarily follows the pattern graph structure.
Interestingly, in many cases this still works quite well because of the quick pruning by the type checks in a well-typed graph.
In addition, often the rules match from some parameter nodes onwards, which are typically well-suited starting points for searching.

Beware: the \texttt{analyze} command is costly.
It helps in improving the matching performance, but requires execution time on its own.
Don't use it freely.

It is costly because it has to visit all the nodes and edges in the graph,
in order to gather statistical data about the amount of breadth-splitting per node and edge-types. 
For each $(NodeType, EdgeType, NodeType\ast)$-triple (for all node and edge types), it counts for each start node of $NodeType$ the number of edges of $EdgeType$, which are leading to an end node of $NodeType\ast$.
Given that information, search planning is able compute a search plan that avoids to follow structures splitting into breadth (V-structures) that occur often in the host graph.
Given the information about the element counts per type (which is directly available from the host graph), 
search planning is able to compute a search plan that avoids lookups on populated types.

Such a dynamically generated search plan typically leads to minimum matching durations, but not always -- sometimes you may fare better by manually assigning priorities to the pattern elements, thus influencing the search order in the initial static search plan (high-prio elements are matched first).

Again: Use types!
The more fine grain a graph is typed, the better are the statistics regarding the breadth-splitting and the number of elements of a certain type, and the better are then in consequence the search plans in their ability of evading breadth-splitting structures and avoiding lookups of often occurring types.

%-----------------------------------------------------------------------------

\subsection{Consequences For Optimization, II}

The consequences of the attribute indices~\ref{sec:attributeindices} and incidence count indices~\ref{sec:incidencecountindices} for optimization are: TODO.

\subsection{Consequences For Optimization, III}

The consequences of the name index~\ref{sec:nameindex} and the uniqueness index~\ref{sec:uniqueness} for optimization are: TODO.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Location Passing and Memorization}
Often in a transformation, you know the location that needs processing from a previous step.
In this case, return these locations out from the rules, store them in variables of node (or edge) type, and hand them in again via rule arguments to the follow-up rules.
Or call embedded rules, directly handing in elements from the containing rule, saving you the intermediate assignments.

Commonly, this is just the right way of handling this situation, 
as you must continue processing the very spot you just processed with the previous rule(s) to achieve a valid result.
Sometimes you don't need to, but gain higher performance when you do so.
Optimize this way then, use \emph{rooted} pattern matching, with roots defined by previous rules, it's cheaper as it saves you the lookup in the graph.

A different way of parameter passing is adding reflexive edges to the graph to mark the spot of processing.
This is normally working well, too, but the parameter passing is typically easier and does not "pollute" the graph with processing information.

In case of a statically not known number of locations as they appear e.g. in a wavefront algorithm, you may store the locations in storages (cf. chapter \ref{cha:container}), i.e. collection valued variables (which are iterated over in the sequences or passed to the rules as \texttt{ref} parameters).
Search will then start at these parameters, instead of looking up a value in the graph by type (unless search planning taking the statistics about the graph into account comes to the conclusion that a lookup on a super-seldom type is still the better approach).

A sophisticated way of remembering facts about non-local properties is to compute them with data flow analyses (see section \ref{subsub:flow}) and store them as attributes in the graph elements.
This allows to replace searching for distant values, or global properties like reachability, by checking a local property, at the price of re-running an analysis every time the graph changes in an important way.

Variables and storages can be seen as indices into the graph, indices that are typically more selective than the automatically supplied indices; 
but you must maintain them by hand in contrast to the automatically supplied ones.
Beware of elements already deleted from the graph still hanging out in your storage because you forgot to remove them.

The approach of remembering state instead of searching when needed has a clear caveat: the code becomes susceptible to ordering effects (more brittle) and less readable. 
As in normal programming, you must balance performance optimizations against maintainability.

\subsubsection*{Traversal passes}

In traditional graph programming, 
typically functionality is smeared into (generic) graph traversal code of global passes (and orchestrated via processing state), carried out by recursive visiting calls.
One pass taking care of different aspects in the performance optimized version,
for each aspect its own pass in the easier to maintain and understand version.
Index structures or state in helper variables may be used to get quicker to the parts of interest.

In graph rewriting with local rules, index structures are a must, otherwise each rule application would amount to a traversal pass (an extreme waste of performance).
In pure graph rewriting, they would be queried with every rule application.
In a performance optimized -- and ofter easier to specify version -- parameters are used instead to process a certain spot in the graph.
The price for these benefits is the need to synchronize the processing state with the graph state (to prevent stale elements after graph changes), or the computations with the processing state (all using rules have to be adapted when the processing state is changed).

\begin{note}
An aspect of the notion of graph rewriting is that no state is stored outside of the graph (a similar idea applies to database design), 
with every rule application, the graph is queried (/searched) for the parts of interest.
The benefit is that no stored state (elementary variables, storages, match-arrays) can get stale at changes,
which excludes an entire class of hard-to-find errors,
but it comes at the price of repeated searches to get again to (/find) the parts of interest.
With its indices, \GrG~is well suited to this style of programming, as getting to the part of interest is quick, but when beneficial or needed, you can revert to a more imperative programming style based on an intermediate processing state.
\end{note}

\pagebreak

\begin{note}
If a simple specification is fast enough, keep it simple.
If the program executes fast enough, let it carry out unneeded work.
You must weight programming time esp. including the long term costs of maintenance against execution time.
Be as declarative as possible, become only as imperative as needed.
This may mean not to use state memorization and passing for faster execution, but to parallelize pattern matching instead (see following section).
\end{note}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Profile and Parallelize}\label{sec:performanceparallel}

A basic and often sufficient means of profiling is built into GrShell:
After each sequence execution, the time required to carry it out is printed.

Sometimes you need further information about what's going on.
Use the built-in profiling\footnote{You may of course apply a profiler on the generated code, but then you descend to the level of implementation of \GrG} then, by specifying the \texttt{profile} compiler option (cf. \ref{grgenoptions}) to request search code instrumentation, or better the \texttt{set profile on} shell command (cf. \ref{sec:compilerconfigshell}).
You receive two kinds of results when executing matchers instrumented this way.

\begin{enumerate}
	\item After each sequence execution, the shell prints out the number of search steps that were carried out, in addition to the time. 
		A search step consists of binding a graph element to a pattern element.
	\item You can request a detailed per-action profile with the \texttt{show profile} shell command (cf. \ref{grsthings}).
\end{enumerate}

The detailed per-action profile contains a value that gives you a hint regarding the expected use of matcher parallelization, the higher the number the better is the rule suited.

The means available to parallelize -- hopefully boosting performance -- are described in Chapter~\ref{cha:parallel} in detail.
These are:
\begin{itemize}
	\item parallel action matching with an annotation,
	\item parallel isomorphy checking with a dedicated operation,
	\item parallel execution in the sequences (explicitly programmed).
\end{itemize}

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compilation and Static Knowledge}

\subsubsection*{Use saved graph analysis data}
The different instance graphs for a certain graph-based problem you work with often show similar characteristics regarding the types and their connectedness.
You can analyze such a characteristic graph, once, and save the results,
with the \texttt{custom graph statistics save} command, cf. \ref{custom}.
And build the static matchers based on it, at each following static action generation, by loading that statistics file. 
This is possible with the \texttt{statistics} compiler option, cf. \ref{grgenoptions},
and with the shell \texttt{new set statistics} command, cf. \ref{sec:compilerconfigshell}. 
It is seldom that up-to-the-point dynamic information about the host graph makes a real difference,
while graph analysis and matcher re-generation at runtime are \emph{costly} -- push this effort from runtime to compilation time.

\subsubsection*{Use Compiled Sequences}
The compiled sequences from the rules file are executed a good deal faster than the interpreted sequences from the shell.
So if you have to optimize for performance, replace interpreted sequences by compiled ones.
The price you pay is a loss of debuggability, a compiled sequence can only be executed as one big step 
(the introduction of subrule debugging -- see \ref{secdebuggersubrule} -- improved on this, but the differences regarding state introspection and control are still considerable).

\subsubsection*{Use Pre-Compiled Code}
For shortly-running tasks the JIT-compiling overhead dominates the execution runtimes.
The times in the range of a few dozen milliseconds printed out for most of our tests are the times needed for just-in-time compilation of the .NET bytecode of the matchers into machine code --- the matching itself is typically several orders of magnitude faster, it is in the range of microseconds or below (for the small example graphs and simple patterns used).
This effect can be seen when a sequence takes some hundred milliseconds to execute when executed for the first time, but afterwards completes immediately (for a task of the same size).
You may cut down on this overhead by utilizing the \texttt{ngen} tool of .NET for pre-compiling the \GrG-dlls (the supplied as well as the generated ones, cf. \ref{systemoverview}), or the \texttt{--aot} option of mono, for ahead-of-time compilation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Miscellaneous Things}

Besides the things discussed in the following, you may employ compiler flags to disable certain features evading their performance impact (e.g. debugging of embedded execs), cf. Subsection~\ref{grgenoptions}; they can be also configured in the \GrShell{}, then also firing of the action/debug events in the interpreted sequences gets disabled, see Section~\ref{sec:compilerconfigshell}. 

\subsubsection*{Visited Flags versus Storages versus Reflexive Edges}
Visited flags are the most efficient way of marking elements, in case a large number of elements has to be marked, or if all elements -- irrespective whether marked or not -- need to be iterated.
(This holds because they are stored in a flags variable of the graph elements themselves (the first $k$ flags, afterwards they need to be stored outside of the graph)).

Otherwise they are inefficient because they do not allow to access(/lookup) the marked elements based on the marking information ---
a lot of elements need to be iterated for a lookup, just to filter the visited ones out.
Storages that allow you to access their contained elements quickly are better then.

An alternative are reflexive marker edges of a special type in the graph, search planning favors them (assuming they appear in much smaller quantities than normal edges), so search starts at those elements. 
Besides they can be visualized directly in the graph (emulating a kind of "cursor").

\subsubsection*{Loops versus All-Bracketing versus Iterated}
Regarding performance, you should prefer all-bracketing (cf. \ref{sec:ruleapplication}) to sequence loops and those to iterated patterns.
Applying a rule on all matches is the most efficient way of processing multiple spots in a graph.
This holds for normal matchers, but even more so for parallelized matchers.
For a normal rule call, typically a part of the concurrent search effort is wasted because some workers already passed the point where the match was found; 
when all matches are sought, no effort is wasted -- and typically the entire search takes longer in that case, which lowers the break-even point regarding the overhead incurred by the parallel matcher.

Loops are not much slower due to the search state space stepping, continuing where the last iteration left of.
But they are semantically considerably different: each step of the loop operates on the then-changed graph, in contrast to applying a rule on all matches available at a certain point in time.
If you have to explore new match possibilities created by applying the rule you need a loop, but even then you'll likely benefit from applying an all-bracketed rule in the loop.
On the other hand you have to use a loop in case the parts of the matches which are to be modified, e.g. retyped, are not disjoint (an element can only be changed once).

Iterated patterns are similar to all-bracketed rules insofar that they match all spots in the graph existing at a certain point in time.
They can be embedded directly in a rule (this is considerably less heavyweight than declaring a rule and passing the attachment points as parameters), and they allow for easy yielding of elements to the pattern (typically with an accumulating yield; this is also less heavyweight than returning elements back to a calling rule).
But the overhead of the three pushdown machine must be payed for them, and they can't be matched in parallel.
Using an iterated as the outermost block of a rule is wasteful, all-bracketing is to be preferred in this case.
But don't replace iterateds unless really needed, they are simply much more convenient.

\subsubsection*{Reachable versus Subpattern Recursion}
Prefer the reachable predicate over subpattern recursion.
The reachable predicate has a tight implementation, while subpattern recursion must pay for graph parsing with the three pushdown machine (whose execution overhead is not horrible but can be clearly felt).

Note the semantic difference: elements already matched in the pattern cannot be matched again in case of subpattern recursion due to the isomorphy constraint. 
The reachable predicate does not know about the pattern it is called from, so elements already matched in the pattern can get matched again.
This is often what is wanted anyway, and leads together with the increased convenience of having only to call a pre-implemented functionality instead of having to program an iterated path to the clear advice to favor the reachable predicate.
You have to use subpattern recursion, though, if you want to impose a certain pattern of type alternation on the iterated path, 
or if you want to match more than chains of single nodes linked by single edges (esp. attached tree-like structures), 
or if you want to somehow change the elements on the path -- not searching merely for existence.

\subsubsection*{Helper Programs versus Patterns}
\GrG\ search planning can be compared to searching straw stars on a freshly harvested field,
looking at the places where the ground is only slightly covered, 
only reaching into the haystacks when they can't be circumvented at all, and only for peripheral parts.
A pattern matcher is generated based on the assumption that search planning worked well in circumventing those haystacks. 
It is based on nested loops for binding the pattern elements to the graph elements;
and for a pattern node that is reached via multiple pattern edges
on comparison code to check
whether the node bound to the pattern node with the first loop
is the same as the node that is reached later on from the other parts. 

%\pagebreak

That approach works very well for sparse graphs with low incidence counts.
But for graphs with massively linked parts that need to be reached on different paths you may need to overwrite this behaviour with hashset based connectedness checks.

The comparison code as such is a simple object identity comparison, so it is very cheap, 
but what weights in here are the cycles needed to iterate through the many different edge candidates until one is found that indeed is incident to the current node candidate.
Hashset based connectedness checks are a good deal more expensive than iteration and reference comparison (esp. due to hashset building and filling), but they can be done in $O(1)$ in contrast.
For a handful of elements, they straight loose against the default code emitted by \GrG, but when we speak of hundreds or thousands of elements, they win.

So when you have to find all nodes that are adjacent to two different nodes at the same time, 
and both of those have a considerable amount of adjacent nodes in addition,
you should switch to the following approach:
store the adjacent nodes of one node in a hash set, and query it with the adjacent nodes from the other node.
This is only $O(n)$ instead of $O(n*n)$ due to the $O(1)$ hash set lookup (applied as query, or implicitly used in a hash set intersection).

This especially holds when you are only interested in the fact whether a certain minimum number is reached -- in that case you may write a helper function that returns as soon as this amount is reached; see \cite{MovieDatabase} for an example of this.
Using a helper function is also commonly more lightweight than using a helper rule (in case the helper function does not get large, which is the case when a pattern containing more than a handful of elements is needed).

\subsubsection*{Helper Programs versus Patterns on an Example}

In the example \cite{MovieDatabase} mentioned above which is solving the TTC14 Movie Database Case \cite{MovieDatabaseCase}, we started optimizing the rule shown in Fig. \ref{fig:findCouples.grg}
with the search plan printed by the \texttt{explain} command (cf. \ref{custom}) in Figure \ref{fig:InitialSP}.

\pagebreak

\begin{figure}[hp]
	\begin{verbatim}
findCouples:
    lookup -_edge0_inlined_idpt_0:personToMovie-> in graph
    from <-_edge0_inlined_idpt_0- get source pn1:Person
    from -_edge0_inlined_idpt_0-> get target m1_inlined_idpt_0:Movie
    from m1_inlined_idpt_0 incoming <-_edge1_inlined_idpt_0:personToMovie-
    from <-_edge1_inlined_idpt_0- get source pn2:Person
    independent {
        (preset: pn1)
        (preset: m1 after independent inlining)
        (preset: pn2)
        (preset: _edge0 after independent inlining)
        (preset: _edge1 after independent inlining)
        from pn1 outgoing -_edge2:personToMovie->
        from -_edge2-> get target m2:Movie
        from pn1 outgoing -_edge4:personToMovie->
        from -_edge4-> get target m3:Movie
        from pn2 outgoing -_edge3:personToMovie-> check m2 connected to _edge3
        from pn2 outgoing -_edge5:personToMovie-> check m3 connected to _edge5
    }
\end{verbatim}
	\caption{Initial search plan}
	\label{fig:InitialSP}
\end{figure}

\begin{figure}[h!p]
	\lstinputlisting[language=LANGgrgen]{resources/findCouples.grg}
	\caption{findCouples rule}
	\label{fig:findCouples.grg}
\end{figure}

The graph is entered with a lookup of a \texttt{personToMovie} edge, then the candidates for the source node \texttt{pn1} and the target node \texttt{m1} are extracted from it.
Afterwards, the other \texttt{personToMovie} edge is matched in reverse from the movie to \texttt{pn2}.
Several elements in the \texttt{independent} are handed in as already matched presets from the outer pattern, 
then the \texttt{personToMovie} edges are taken from \texttt{pn1} to the movies \texttt{m2} and \texttt{m3}, 
and finally the \texttt{personToMovie} edges from \texttt{pn2} are matched, with an implicit check that the target movie is the same as the one already matched.

We see here an automatically applied optimization, the movie \texttt{m1} was inlined from the independent pattern to its containing pattern.
Without this optimization, \texttt{pn1} and \texttt{pn2} would have to be enumerated in the main pattern \emph{unconnected}, resulting in the unfolding of the cartesian product of all \texttt{Person} nodes 
--- before handing it in to the matcher of the nested independent pattern in order to purge the actors without a connecting movie.

Note that connectedness checks for nodes that are reached via multiple edges are carried out as early as possible.
In the example search plan, take a look at the two latest lines:\\
\verb#from pn2 outgoing -_edge3:personToMovie-> check m2 connected to _edge3#\\
\verb#from pn2 outgoing -_edge5:personToMovie-> check m3 connected to _edge5#\\
For those two lines, after the edges were matched, there are no get target operations to fetch \texttt{m2} and \texttt{m3} existing, because the targets \texttt{m2} and \texttt{m3} were already matched from other edges on.
Instead, the connectedness check is carried out here:
it is checked that the target node of \verb#_edge3# is equal to the target node already bound to \texttt{m2}, and that the target node of \verb#_edge5# is equal to the target node already bound to \texttt{m3}.

\subsubsection*{Helper Programs versus Patterns on an Example, Optimized}

In the example case \cite{MovieDatabase} we applied a succession of optimization steps.
The final optimized rule is shown in \autoref{fig:findCouplesOpt.grg}, and its helper functions in \autoref{fig:atLeastThreeCommonMovies.grg} and \autoref{fig:getCommonMovies.grg}.
Its search plan is listed in \autoref{fig:FinalSP} below.

\begin{figure}[hptb]
	\begin{verbatim}
findCouplesOpt:
    parallelized lookup pn1:Person in graph
    if { depending on findCouplesOpt_node_pn1 }
    from pn1 outgoing -p2m1_inlined_idpt_0:personToMovie->
    from -p2m1_inlined_idpt_0-> get target m1_inlined_idpt_0:Movie
    from m1_inlined_idpt_0 incoming <-p2m2_inlined_idpt_0:personToMovie-
    from <-p2m2_inlined_idpt_0- get source pn2:Person
    if { depending on findCouplesOpt_node_pn2 }
    if { depending on findCouplesOpt_node_pn1,findCouplesOpt_node_pn2 }
    independent {
        (preset: pn1)
        (preset: m1 after independent inlining)
        (preset: pn2)
        if { depending on findCouplesOpt_node_pn1,findCouplesOpt_node_pn2 }
        (preset: p2m1 after independent inlining)
        (preset: p2m2 after independent inlining)
    }
    \end{verbatim}
	\caption{Final search plan}
	\label{fig:FinalSP}
\end{figure}

Note the parallelized first lookup that is spreading matching work over multiple threads.
In addition, the computation of the common movies is executed in parallel, materializing the results to a common movies set, per match.
Only a part of the original pattern is used here, because the connectedness check was moved to a helper function, called from an attribute condition.

\begin{figure}[hptb]
	\lstinputlisting[language=LANGgrgen]{resources/findCouplesOpt.grg}
	\caption{findCouplesOpt rule}
	\label{fig:findCouplesOpt.grg}
\end{figure}

\begin{figure}[hptb]
	\lstinputlisting[language=LANGgrgen]{resources/atLeastThreeCommonMovies.grg}
	\caption{atLeastThreeCommonMovies rule}
	\label{fig:atLeastThreeCommonMovies.grg}
\end{figure}

\begin{figure}[hptb]
	\lstinputlisting[language=LANGgrgen]{resources/getCommonMovies.grg}
	\caption{getCommonMovies rule}
	\label{fig:getCommonMovies.grg}
\end{figure}

\pagebreak

\subsubsection*{Helper Programs versus Patterns on an Example, Profiling}

When you enable profiling for the \verb#imdb-0005000-50176.movies.xmi# example, cf. \ref{sec:performanceparallel}, you see a drastic reduction in search operations for the optimized version compared to the non-optimized version (and a corresponding reduction in runtime), compare \ref{fig:profilefindCouples} to \ref{fig:profilefindCouplesOpt}. 

Execution was carried out on a double core.
The number of matches differs because automorphic matches filtering was executed during matching in one case, and after matching in the other case.
Also, in the optimized version there are no matches occurring from an embedded \emph{exec}, as all search work was shifted to the parallelized matcher.

\begin{figure}[hptb]
	\begin{verbatim}
Executing Graph Rewrite Sequence done after 23174.7 ms with result True:
 - 138718180 search steps executed
 - 34958 matches found
 - 34958 rewrites performed
> show profile findCouples
profile for action findCouples:
  calls total: 1
  steps of first loop of pattern matcher total: 5000
  search steps total (in pattern matching, if checking, yielding): 136430986
  search steps total during eval computation): 0
  search steps total during exec-ution (incl. actions called): 2287194
  search steps until one match: 0.00
  loop steps until one match: 0.00
  search steps per loop step (until one match): 0.00
  search steps until more than one match: 136430986.00
  loop steps until more than one match: 5000.00
  search steps per loop step (until more than one match): 4441.29
  parallelization potential: 474.49
  \end{verbatim}
	\caption{Profile of findCouples}
	\label{fig:profilefindCouples}
\end{figure}

\begin{figure}[hptb]
	\begin{verbatim}
Executing Graph Rewrite Sequence done after 1764.0 ms with result True:
 - 2962438 search steps executed
 - 17479 matches found
 - 17479 rewrites performed
> show profile findCouplesOpt
profile for action findCouplesOpt:
  calls total: 1
  steps of first loop of pattern matcher total: 5000
  search steps total (in pattern matching, if checking, yielding): 2962438
  search steps total during eval computation): 0
  search steps total during exec-ution (incl. actions called): 0
for thread 0:
  search steps total: 373345
  loop steps total: 2193
  search steps until one match: 0.00
  loop steps until one match: 0.00
  search steps per loop step (until one match): 0.00
  search steps until more than one match: 373345.00
  loop steps until more than one match: 2193.00
  search steps per loop step (until more than one match): 33.64
for thread 1:
  search steps total: 374142
  loop steps total: 2807
  search steps until one match: 0.00
  loop steps until one match: 0.00
  search steps per loop step (until one match): 0.00
  search steps until more than one match: 374142.00
  loop steps until more than one match: 2807.00
  search steps per loop step (until more than one match): 15.91
  \end{verbatim}
	\caption{Profile of findCouplesOpt}
	\label{fig:profilefindCouplesOpt}
\end{figure}

