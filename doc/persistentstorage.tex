\chapter{Persistent Storage}
\label{cha:persistentstorage}

The support for peristent storage can be distinguished into
\begin{itemize}
	\item the import/export functionality, that allows to serialize/deserialize an in-memory graph at a dedicated point in time
	\item the persistent graph, which is an in-memory graph mirrored to a database (and mirrored back at opening, by a persistence provider), changes to it are written through to the database
	\item the record/replay functionality, that lies in between the two, starting with a full graph dump to a \texttt{.grs} file, then persisting ongoing changes to the \texttt{.grs} file.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Persistent Graph Creation}
\label{persistentgraphcreationcommands}

The \texttt{new graph} command (cf. \ref{graphcreationcommands}) may be extended to create or open a \indexed{persistent graph}.

\begin{rail}
  PersistenceSuffix: 'persist' 'with' Text 'to' Text (Text?);
\end{rail}\ixnterm{PersistenceSuffix}\ixkeyw{persist}\ixkeyw{with}\ixkeyw{to}
The first \emph{Text} specifies the persistence provider to use, which has to be given as a dll filename.
With \texttt{libGrPersistenceProviderSQLite.dll}, the database (management system) SQLite is used; this dll is as of now the only offered persistence provider.
The second \emph{Text} specifies the database connection string to use, which must specify the name of the database file to use, and should set the version.
The optional third \emph{Text} may specify additional persistent graph parameters.

\begin{example}
The command
\begin{grshelllet}
new graph "persistentgraph/persistentgraph_attributed"\
persist with "libGrPersistenceProviderSQLite.dll"\
to "Data Source=testgraphfilename_attributed.db;Version=3;"
\end{grshelllet}
creates a new graph from the model/actions specified by the \texttt{persistentgraph\_attributed.grg} file, persisting it with the persistence provider \texttt{libGrPersistenceProviderSQLite.dll} to the \texttt{testgraphfilename\_attributed.db} database file.
The command is from the example \texttt{persistentgraph\_attributed.grs}, it can be found in the \texttt{tests/persistentgraph} directory.
\end{example}

\begin{warning}
When using a persistent graph, you are strongly advised to employ persistence provider aka database transactions, at least when non-minor graph or attribute changes occur.
Executing a write-heavy sequence without an enclosing database transaction which causes the implicit use of many small transactions is about 500 times slower than the use of an enclosing database transaction.
See \ref{sec:pptransactions} for an explanation of the relevant constructs in the sequences language.
\end{warning}

\begin{note}
The persistent graph also has its price: executing a write-heavy sequence with nearly no matching on a persistent graph with an enclosing database transaction is about 15 times slower than executing the sequence on an in-memory only graph.
\end{note}

\subsection*{Persistent Graph Design and Consequences}
The persistent graph is implemented in large parts by a persistence provider, currently only the SQLite based \texttt{libGrPersistenceProviderSQLite.dll} is available (the \texttt{LGSP\-Persistent\-Named\-Graph} implementing the \texttt{INamedGraph} interface from \texttt{libGr} is only a very thin extension of the \texttt{LGSPNamedGraph}, the real work is delegated to its persistence provider). 

The workflow is: at session begin, when the persistent graph is created, the database is read (or created if it does not exist yet), then ongoing changes to the graph are persisted to the database, until the session ends.
At the begin of the next session, the state at the end of the previous session is still available.

Matching/reading occurs entirely in-memory, the database is read only once, when the persistent graph is loaded.
The writing of changes (to the graph structure as well as to the attributes of its entities) is realized by listening to the graph change events (and writing small change sets, typically single values, even in the case of containers).
This design allows for quick pattern matching and for quick update writing, but comes at the price of slow loading (due to initial cleanup/container change integration at loading, and esp. compared to database-only operations with on-demand reading/writing, or a lazy loading/caching approach).

A mark and sweep like garbage collector is employed directly after the inital graph reading, it finds out which of the known objects and graphs are still reachable from the host graph on, and which of the zombie nodes/edges are still reachable from the host graph on.
The ones that are not referenced anymore are deleted.
Also container compactification occurs: container attributes are stored as a log of change commands (like add and rem), at initial loading, the changes are replayed, and the by-then reached most current state is written anew.

Note that only the objects that are reachable from the host graph on are stored, in contrast to all the objects created, even if they are accessible from e.g. sequence variables.
Nodes/edges that are deleted or retyped turn into zombies, references to them dangle afterwards.
They may still be available in attributes of node/edge type or attributes of container of node/edge type (in nodes or edges or internal class objects).
These still referenced zombies are reported as warnings at persistent graph loading.
The attribute containing them should have been assigned null or they should have been removed from the container attribute containing them before the removal from the graph or the retype (a retype keeps the position, name, and unique id, but the retyped element has a different .NET object identity than the previous element).

The persistent graph as it is implemented by the SQLite persistence provider is constrained in certain ways compared to the regular graph:
\begin{itemize}
	\item when attributes are used that contain nodes/edges (graph element references), the graph elements of the graph must reference the containing graph, i.e. \texttt{node edge graph;} must have been declared (cf. \ref{sec:graphnesting}) -- in exchange, the persistent graph is capable of coping with references to graph elements from other graphs than the one the reference is contained in -- this is \emph{not} supported by the GRS exporter/importer
	\item the persistent graph does not support the reuse optimization (cf. \ref{sec:graphcustomcommands}, \texttt{optimizereuse} that recycles deleted elements)
	\item the persistent graph does not support parallel sequence execution (cf. \ref{sec:sequenceparallelization})
	\item external attribute types (cf. \ref{sub:extcls}) are not supported (the external emitting and parsing (cf. \ref{sub:extemitparse}) of the GRS import/export is not supported)
\end{itemize}

The SQLite persistence provider uses the following table mapping: the graph structure is stored in topology tables (\texttt{nodes}, \texttt{edges}, \texttt{graphs}, \texttt{objects}) with one row per entity, the entities (nodes, edges, objects) are stored as rows in per-type table with the attributes as columns, besides container attributes, which have their are own tables (additionally, type information is stored in \texttt{types} and \texttt{attributeTypes} tables, plus a bit of metadata in the \texttt{metadata} table).
A minimal amount of database indices are used (initial reading occurs by full table scans), as pattern matching is not carried out against the database -- the indices to achieve fast lookups and matching are all kept in-memory, the same runtime structures are (re-)used that are used for a non-persistent graph.

You could change the persistent graph offline by carrying out SQL commands, even though this is discouraged (strongly, unless you know what you are doing).

\subsection{Model Update}\label{sec:modelupdate}

Before the graph can be loaded from the database, it may be necessary to update the model stored in the database to the model used by the current host graph (the graph in the database adheres to the (meta) model from the database, not necessarily the current (meta) model from the model file/assembly; a loading is only possible in case of a compatible new model/compatible changes).

The persistence provider checks for changes by comparing the models, and depending on them
\begin{itemize}
	\item aborts the update,
	\item asks for user confirmation,
	\item auto-updates (or initializes),
	\item does nothing.
\end{itemize}

In case entity types were removed from the model that still have instances in the database, and/or enum types were removed from the model that still appear as types of attributes in the database, the update is aborted (and the database left unchanged) --- this behavior prevents an accidental loss of data.
The update is also aborted when types have changed their kind and the aforementioned conditions apply, e.g. a \texttt{node class Foo} was switch to an \texttt{edge class Foo} (and the database contains instances of its type), or an enum \texttt{Bar} was switched to an object \texttt{class Bar} (and the database contains values of its type, determined by attributes of its type), they are modeled as a type deletion in a first step, followed by a type introduction in a second step (types are not only defined by their name, but also their kind (but not their content, content changes are handled as single attribute changes)).
You have to revert to an old model in order to delete the instances or the attribute of their types, before you can proceed.
Then you can also migrate the data you don't want to loose, see \ref{modelmigration} for more on this topic.
Removing entity types that don't have instances or enum types that don't appear as types of attributes in entity types is safe and carried out automatically (because you only loose the corresponding meta data/type information).

As an aide for model migration (if you don't have access to the original model anymore), the model stored in the database is written to a (temporary) model file in case of an abort (whose name starts with \texttt{partial} and contains the database name) -- but note, that this is \emph{not} the original model, for one is it only a partial model (lacking e.g. methods or indices), for the other is the inheritance hierarchy flattened, all classes contain all the attributes from their supertypes, lacking the extends specification.

\begin{warning}
An update may fail directly after you deleted all instances of relevant nodes/edges.
You have to open the graph first and wait for completion of garbage collection, then you can delete the types.
The next time you open the graph, the model will be updated (successfully).
\end{warning}

\begin{note}
We speak of model updates, while in fact, you may also open a graph with a different model, by-accident or intentionally.
Still, it is seen as if you modified the previous model to the current state, i.e. the difference from the database to the current models is computed and acted upon (in order to synchronize the database to the current model).
\end{note}

In case attributes or enum cases were deleted, the user is asked to confirm the update, which would remove besides the attribute from the type also all the attribute values in the entities of the type stored in the database, potentially causing data loss, or move the persistent graph to a broken state, if the enum cases still appear in attributes (as attribute values).
This holds also for attributes that changed their type, e.g. an attribute \texttt{bar:int} being turned into an \texttt{bar:string}, in the same \texttt{class Foo}, they are modeled as an attribute removal in a first step, followed by an attribute introduction in a second step.
When the user accepts the update, it is carried out and the attributes are gone for good, their contained values lost, and the enum cases removed -- it may be the graph is in an unreadable state thereafter (unless further measures are taken), if the removed enum cases are still contained in attributes (see \ref{modelmigration} for how to handle this).
When the user denies the update, it is aborted, like in the case of deleted entity types with instances or deleted enum types with attributes, and the graph is neither changed nor opened (so that you can roll back to an older model state and carry out the required migrations).
The interactive querying of the user can be prevented by \emph{PersistentGraphParameters}, they will be introduced later in the section.

In case of changes that are only extensions, the update will be automatically carried out.
This esp. holds when new entity or enum model types were introduced
(they will be added to the database model and can be used thereafter),
when new attributes were introduced to entity types
(they will be initialized to their \GrG{ } default values in the instances of their containing entity types in the database (\emph{not} to the default initializations specified in the \GrG{ } model file, cf. \ref{sct:attrtypes})),
and when new enum cases were introduced (thereafter, they can appear in attributes whose type is that enum type).
For kind changes of types and type changes of attributes, 
the second step of introducing the new types of the name is such a safe extension,
after the first step of deleting the old type of the name passed.

Upon first opening of a persistent graph, the database will be initialized to the model from the (current) graph.

In case the model is unchanged compared to the last time the database was opened, or only parts without relevance to persistent storage (like indices, which are memory-only) changed, the graph is simply read (this is of course also the follow-up step after the automatic changes and the confirmed changes were carried out).

\subsubsection*{Persistent Graph Parameters}
\label{persistentgraphparameters}

\begin{rail}
  PersistentGraphParameters: ('update/entitytype.attr' | 'update/enumtype' | 'update' | 'initializeonfailure/enumtype')+(';');
\end{rail}\ixnterm{PersistentGraphParameters}

The persistent provider supplying the persistent graph can receive inputs in the form of persistent graph parameters, the ones that are currently supported allow to influence the model update.

The \texttt{update} parameters allow to exclude attributes or enum cases from the update the user has to confirm.
The parameter \texttt{update/entitytype.attr} causes an auto-confirmation (and thus update) of the deletion/type changing of the attribute \texttt{attr} from graph element or class object type \texttt{entitytype} during persistent graph opening.
The parameter \texttt{update/enumtype} causes an auto-confirmation (and thus update) of the enum type \texttt{enumtype} for all deleted enum cases.
The parameter \texttt{update} causes an auto-confirmation (and thus auto-update) of all attribute deletions and type changes and enum case deletions (but is potentially dangerous and thus not recommended).
The parameter \texttt{initializeonfailure/enumtype} causes an initialization of the attributes of type \texttt{enumtype} that fail at loading because their enum case is not available anymore with the default value of the enum type (the first case of value 0 which is typically the first case in the enum specification list, or the first case from the enum specification list if no enum case of value 0 exists).

These parameters allow to deploy a model migration script including a silent model update to user machines (see \ref{modelmigration} for more on model migration).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Graph Input and Output}
\label{outputcmds}

\begin{rail}
  'save' 'graph' Filename
\end{rail}\ixkeyw{save}\ixkeyw{graph}
Dumps\indexmain{dumping graph} the current graph as \GrShell\ script\indexmain{graph rewrite script} into \emph{Filename}.
The created script includes
\begin{itemize}
  \item selecting the backend
  \item creating a new graph with all nodes and edges (including their persistent names)
  \item restoring the (graph global) variables
  \item restoring the visualisation styles
\end{itemize}
but not necessarily using the same commands you typed in during construction.
Such a script can be loaded and executed by the \texttt{include} command (see Section~\ref{inclcondexeccommands}).

\begin{rail}
  'export' Filename ('.grs' | '.grsi') ('.gz')? \\ ( () | 'nonewgraph') (('skip/type.attr')*)
\end{rail}\ixkeyw{export}\indexmain{export}
Exports an instance graph in GRS (.grs/.grsi) format, which is a reduced \GrShell\ script
(it can get imported and exported on API level without using the \GrShell, see Section \ref{sub:imexport}).
This is the recommended standard format (it is lightweight, human-readable and editable, and supported by an optimized importer).
The file contains a \texttt{new graph} command, followed by \texttt{new node} commands, followed by \texttt{new edge} commands.
If the \texttt{.gz} suffix is given the graph is saved zipped.
The export is only complete with the model of the graph given in the \texttt{.gm} file.
Exporting fails if the graph model contains attributes of \texttt{object}-type; you may add support for storing them, too, see \ref{sub:extemitparse} for more on this.

When the optional parameter \texttt{nonewgraph} is given, the initial \texttt{new graph} command at file begin is omitted.
Such a file cannot be \texttt{import}ed, but only \texttt{include}d, as it is incomplete.
You have to ensure an empty graph of correct model exists before you can include a file exported this way.

The skip parameters allow to exclude attributes from the node/edge attribute initializer lists of the \texttt{new node} or \texttt{new edge} commands.
The parameter \texttt{skip/type.attr} causes omission of attribute \texttt{attr} from graph element type \texttt{type} during graph serialization.
This way you can export an again importable graph if you intend to remove some attributes -- otherwise import would fail due to an unknown attribute getting initialized (in the file exported adhering to the old graph model that is not existing in the new graph model).

The native \texttt{.grs} format is the best support format, but still limited as of now regarding the full runtime object graph that can be stored in-memory, it does not support node/edge attributes in the nodes/edges of a graph that reference nodes/edges from another graph, or node/edge attributes in an internal class object that can be reached from different graphs (node/edge attributes are considered an ugly duckling). You must refrain from using them in this manner (at least as of now), which could happen quite easily when you store a subgraph created by \texttt{insertInduced}/\texttt{insertDefined} in the host graph, with node/edge attributed elements, because the elements are shallowly cloned, referencing the original elements -- it's your task to overwrite them with \texttt{null} or valid versions.

The \texttt{save} command from above is for saving a \GrShell\ session including graph global variables and visualization commands,
the goal of the \texttt{export} command is simply persistent storing of graphs;
esp. for applications that use \GrG{ }to get an algorithmic core, but are not built on the \GrShell.

\begin{rail}
  'export' Filename '.gxl' ('.gz')?
\end{rail}\ixkeyw{export}\indexmain{export}
Exports an instance graph and a graph model in GXL format \cite{GXL,GXL2},
which is somewhat of a standard format for graphs of graph rewrite systems,
but suffers from the well-known XML problems -- it is barely human-readable and editable, and bloated.
It is supported by \GrG{} as exchange format for inter-tool operability.
Exporting fails if the graph model contains attributes of container or \texttt{object}-type.
If the \texttt{.gz} suffix is given the graph is saved zipped.

\begin{rail}
  'export' Filename '.xmi' ('.gz')?
\end{rail}\ixkeyw{export}\indexmain{export}
Exports an instance graph in .XMI format.
XMI files as written by the Eclipse Modeling Framework (EMF) are a standard format in the model transformation community (together with ecore files for the model).
It suffers from the XML problems explained above, in addition it can be even characterized as overly complex and baroque, and it requires some metamodel mapping.
It is supported by \GrG{} as exchange format for inter-tool operability.
The metamodel is assumed to stem from a previous import of an ecore file, with its specific way of mapping \texttt{.ecore} to \texttt{.gm}, i.e. with an underscore prefix, a node type prefix for the edge types, and the \verb#[containment=true]# annotation at the edges that express containment, which is needed so that they are written with XML node containment.
If the \texttt{.gz} suffix is given the graph is saved zipped.

%\pagebreak %force better layout

\begin{rail}
  'export' Filename '.grg' ('.gz')?
\end{rail}\ixkeyw{export}\indexmain{export}
Exports an instance graph in GRG format, i.e. as one GrGen rule with an empty pattern and a large modify part.
There is no importer existing, this format is not for normal use as storage format!
If the \texttt{.gz} suffix is given the graph is saved zipped.

\begin{rail}
  'import' Filename ('.grs' | '.grsi' ) ('.gz')? (ModelOverride)?
\end{rail}\ixkeyw{import}
Imports the specified graph instance in GRS (.grs/.grsi) format (the \emph{reduced} \GrShell\ script,
a \texttt{save}d graph can only be imported by \texttt{include} due to commands not supported by the importer (but an exported graph can be imported by \texttt{include}, too)).
The graph model referenced in the .grs/.grsi must be available as \texttt{.gm}-file.
If a model override of the form \texttt{Filename.gm} is specified, the given model will be used instead of the model referenced in the GRS file.
If a model override of the form \texttt{Filename.grg} is specified, the model(s) of the given rule file will be used instead of the model in the GRS file.
If the \texttt{.gz} suffix is given the graph is expected to be zipped.

\begin{rail}
  'import' Filename '.gxl' ('.gz')? (ModelOverride)?
\end{rail}\ixkeyw{import}
Imports the specified graph instance and model in GXL format.
If a model override of the form \texttt{Filename.gm} is specified, the given model will be used instead of the model in the GXL file.
If a model override of the form \texttt{Filename.grg} is specified(s), the model of the given rule file will be used instead of the model in the GXL file.
The \texttt{.gxl}-graph must be compatible to the \texttt{.gm}-model/\texttt{.grg}-model.
If the \texttt{.gz} suffix is given the graph is expected to be zipped.

\begin{note}\label{shellgxlimport}
Normally you are not only interested in importing a GXL graph (and viewing it), but you want to execute actions on it.
The problem is that the actions are model dependent.
So, in order to apply actions, you must use a model override, which works this way:
\begin{enumerate}
\item \texttt{new graph "YourName.grg"}\\
This creates the model library lgsp-YourNameModel.dll
and the actions library lgsp-YourNameActions.dll
(which depends on the model library generated from the \texttt{"using YourName;"}).
\item \texttt{import InstanceGraphOnly.gxl YourName.gm}\\
This imports the instance graph from the .gxl but uses the model specified
in YourName.gm (it must fit to the model in the .gxl in order to work).
\item \texttt{select actions lgsp-YourNameActions.dll}\\
This loads the actions from the actions library in addition to the already
loaded model and instance graph (cf. \ref{grsthings}).
\item Now you are ready to use the actions.
\end{enumerate}
As of version 3.0beta you can specify a \texttt{.grg} as model override;
basically it does what the given enumeration does.
\end{note}

\begin{rail}
  'import' ((Filename '.ecore')+( )) Filename '.xmi' (Filename '.grg')?
\end{rail}\ixkeyw{import}\label{shellecoreexport}
Imports the specified graph instance in XMI format and the models in ecore format.
They can't be imported directly, as \GrG{ } is not built on EMF.
Instead, during the import process an intermediate \texttt{.gm} is written which is equivalent to the \texttt{.ecore} given -- you may inspect it to see how the content gets mapped.
(The importer maps packages to GrGen packages, classes to GrGen node classes, their attributes to corresponding GrGen attributes, and their references to GrGen edge classes.
Inheritance is transferred one-to-one, and enumerations are mapped to GrGen enums.
Edge type names are prefixed by the names of the node types they originate from to prevent name clashes for references of same name,
and all types are prefixed by an underscore to prevent name clashes with keywords of the rule language.
Edge type declarations are annotated with a \verb#[containment=true]# annotation if they originate from a containment reference.)
After this metamodel transformation the instance graph XMI adhering to the Ecore model thus adhering to the just
generated equivalent GrGen graph model gets imported.
Furthermore, you can specify a \texttt{.grg} containing the rules to apply (including further rule and using further model files -- this way you can use additional custom graph models).
Some examples stemming from old GraBaTs/TTC challenges export XMI with emit statements (e.g. the Program-Comprehension example in \texttt{examples/ProgramComprehension-GraBaTs09}), this is not needed anymore with the built-in XMI export.

\begin{rail}
  'import' 'add' FileSpec
\end{rail}\ixkeyw{import}\ixkeyw{add}
Imports the graph in the specified file and adds it to the current graph
(instead of overwriting the old graph with the new graph).
The \texttt{FileSpec} is of the same format as the file specification in the other import commands.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Graph Change Recording and Replaying}
\label{recordnreplay}

Graph change recording and replaying is available 
\begin{itemize}
	\item for one for post-problem debugging, you can execute your transformation, and after a problem occured inspect the changes that were leading to it
	\item for the other for ensuring persistence of changes as they happen, in case you are using \GrG\ as an application-embedded in-memory graph-datebase
\end{itemize}

\begin{rail}
  'record' Filename ('.gz')? ('start' | 'stop')?
\end{rail}\ixkeyw{record}\ixkeyw{start}\ixkeyw{stop}\indexmain{record}
The record command starts or stops recording of graph changes to the specified file. If neither start nor stop are given, recording to the specified file is toggled (i.e. started if no recording to the file is underway or stopped if the file is already recorded to).
Recording starts with an export (cf. \ref{outputcmds}) of the instance graph in GRS (.grs/.grsi) format, afterwards the command returns but all changes to the instance graph are recorded to the file until the recording stop command is issued.
Furthermore the values given in the \texttt{record} statements (cf. \ref{recstmt}) from the sequences are written to the recording (this allows you to mark states).
If the \texttt{.gz} suffix is given the recording is saved zipped.
You may start and stop recordings to different files at different times, every file receives the graph changes and records statements occurring during the time of the recording.
Note: As a debugging help a recording does not only contain graph manipulation commands (cf. \ref{mani}) but also comments telling about the rewrites and transaction events which occurred (whose effects were recorded).

\begin{rail}
  'recordflush'
\end{rail}\ixkeyw{recordflush}
Flushes the buffers of the recordings to disk. 
To be called to guarantee persistence if you use \GrG{} as a kind of online database, recording the graph changes while running to a redo log.

\begin{rail}
  'replay' Filename ('.gz')? ('from' Text)? ('to' Text)?
\end{rail}\ixkeyw{replay}\ixkeyw{from}\ixkeyw{to}\indexmain{replay}
The replay command plays a recording back: the graph at the time the recording was started is recreated, then the changes which occurred are carried out again, so you end up with the graph at the time the recording was stopped. Instead of replaying the entire GRS file you may restrict replaying to parts of the file by giving the line to start at and/or the line to stop at. Lines are specified by their textual content which is searched in the file.
If a \emph{from} line is given, all lines from file begin on including this line are skipped, then replay starts. If a \emph{to} line is given, only the lines from the starting point on, until-excluding this one are executed (i.e. all lines from-including this one until file end are skipped).
Normally you reference with \texttt{from} and \texttt{to} comment lines you write with the \texttt{record} statement (cf. \ref{recstmt}) in the sequences, marking relevant states during a transformation process.
An example for record and replay is given in \texttt{tests/recordreplay}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Migration}
\label{modelmigration}

During development, you typically change your graph models (they are called meta models by others -- by editing your model files), and want to continue using your existing graphs with the updated models
(the existing graphs are contained in an exported file or are stored persistently in a database).
Partly this works out of the box (because the graph is compatible to the new model, or because of the update functionality), partly this requires dedicated migration code you have to supply.
Also, you may want to carry out the updates on user machines your graph rewrite system is deployed to.
We explain here how to handle these use cases / give an overview of the workflow.
Note that the model the stored graph adheres to is only implicitly available in case of a serialized \texttt{.grs} file (defined by the types and attributes referenced by the \texttt{new} graph element commands), and only partially available in case of a persistent graph in the SQLite database (it is explicitly available -- albeit in reduced form -- in some dedicated types and attributes tables, and implicitly in the layout of the non-topology tables and the values).

Adding types (\texttt{node class}, \texttt{edge class}, \texttt{class}, \texttt{enum}) to a model or new attributes to types within the model or new cases to enums works out of the box, the old graph is imported/loaded without problems, the new types simply don't have any instances thereafter, the new attributes are simply initialized to the default values of their types (the \GrG{} default values, not the values of explicitly specified initializers from the model as introduced in~\ref{sct:attrtypes}), the new enum cases are not used but just available for usage afterwards.
Deletions of entity or enum types and/or attributes and/or enum cases on the other hand does not work out of the box but require preparation, we specify how to handle them in the following (split by file export and database storage).

When you want to delete entity types from the model, you have to first delete their instances from the graph (by graph rewrite rules or sequences or procedures).
Then, in case of a serialization file, you have to export the graph, can then remove the types from the model, and import the graph (without issues due to the removed type).
In case of a  persistent graph in a database, the database has to be closed (the host graph switched to another one or the shell script exited), then re-opened, and closed again. 
The intermediate re-opening is required so that the garbage collection can remove the deleted entities, which runs only at database opening time but after model updating.
Then, you can drop the type from the model, and open the persistent graph (without issues due to the removed type).

When you want to delete enum types from the model, you have to first remove the attributes of that enum type from the entity types containing them (also by graph rewrite rules or sequences or procedures).
Then, in case of a serialization file, you have to export the graph, can then remove the types from the model, and import the graph (without issues).
In case of a persistent graph in a database, the database has to be closed, then you can remove the enum type from the model, and open the database with the new model (without issues); an intermediate opening is not required in this case. 

When you want to delete attributes from the model (i.e. remove them from their containing type), and work with serialization/deserialization, you have to export first with \texttt{skip} instructions, see Section~\ref{outputcmds}, causing the attributes not to be serialized.
Then you can drop them from the model file and import the serialized file without issues (otherwise, import will fail with an unknown attribute message).
When you work with a persistent graph stored in a database, you can drop the attributes from the model file, and at the following database opening (persistent graph creation), the persistence provider will ask you for a confirmation that the attributes not available in the model anymore can to be deleted.
When confirmed, the database is updated to a version with the attributes dropped, and the graph is loaded (without issues due to the removed attributes).
The change has to be confirmed, because the data from the attributes is lost thereafter.
You may supply skip-like \texttt{update} parameters the the persistent graph at database opening, cf.~\ref{persistentgraphparameters}, which auto-confirm the changes, saving you/the user from the necessity to confirm them
(this is esp. helpful if you want to supply a migration to user machines).

When you want to delete enum cases from their containing enum, you have to first remove all their appearances in attributes of this enum type (in the entities of the entity types that contain these attributes, again with graph rewrite rules or sequences or procedures),
and export the graph when working with serialization/deserialization,
or close the database when working with a persistent graph.
Then you can remove the enum cases from the enum type, and import the exported graph (without issues) or open the database (without issues).
In the latter case, the model updater will ask you to confirm the enum case removal, you can safely accept then.
You may supply skip-like \texttt{update} persistent graph parameters at database opening, cf.~\ref{persistentgraphparameters}, which auto-confirm the changes, saving you from the necessity to confirm them.

In case you forgot to delete before exporting, you will receive enum parsing errors upon import (you could then revert the model / temporarily insert the cases again, or -- even if ugly -- apply some text processing to remove them from the \texttt{.grs} file as a remedy).
In case you forgot to delete the enum case values before removing the enum cases from the model specification file and re-opening the database, and you then confirmed prematurely the changes (manually or by auto-confirming persistent graph parameters), you get a database that cannot be loaded anymore (loading will fail at parsing the enum cases that are still in the database but not declared in the enum anymore).
By specifying \texttt{initializeonfailure} instructions in the persistent graph parameters, cf.~\ref{persistentgraphparameters}, you can make the database load again.
The attributes containing removed enum cases get initialized to the default value of the enum type then.
But only in-memory, the old values stay in the database, you have to overwrite them by assigning some current value to the corresponding enum attributes and close the database so it is in a valid state again
(another -- very ugly -- solution would be to update the attributes in the corresponding tables by SQL statements --- enum attributes are stored as text columns containing the enum case text (at least in the current version)).

A kind change of a type or a type change of an attribute is handled as a deletion followed by an addition, so you have to take care of the deletion part (additions don't cause issues).

In general: if you want to reuse stored graphs when the model changes, you have to take care of the dependencies (supplying migration code that works step-wise along the dependencies).
If you forgot to delete the instances / attributes of a type before deleting the type itself (or instances of enum cases before the enum cases), you have to revert the model or temporarily insert the type (the enum cases) again manually, to sucessfully load the export/database with the model in order to carry out the required deletions (the partial model written by the database could be of help regarding this).
